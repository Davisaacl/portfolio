---
title: "Técnicas de reducción de varianza"
author: "Simulación 2020-II"
date: "13 de octubre de 2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Técnicas de Reducción de varianza

1. Variables antitéticas: aplica variables antitéticas a la integral $\int_0^1 \mbox{sen}(t)^{\cos(t)}\, dt$ y calculen el porcentaje de reducción de varianza si es que hay.


_NB_: El martes comenzamos este ejercicio. Aquí dejo tal cual lo que hicimos, agregando comentarios y más abajo pongo otra manera de hacerlo para el caso de estimar la reducción de varianza. 

Para el método de Montecarlo crudo, lo que hicimos fue lo siguiente:

1. repetimos $N$ veces el siguiente proceso:
  - generamos $n$ uniformes $u_1,\ldots, u_n$
  - calculamos $\hat{\theta}_i^{(n)} = \frac{1}{n}\sum_{i=1}^n h(u_i)$, donde $h(x) = sen(x)^{cos(x)}$.

2. Con los valores $\hat{\theta}_1^{(n)},...\hat{\theta}_N^{(n)}$, calculamos un estimador de la varianza $\widehat{Var(\hat{\theta})} = \frac{1}{N-1}\sum_{i=1}^n(\hat{\theta}_i^{(n)} -\bar{\hat{\theta}}^{(n)})^2$.

Esto es lo mismo en código:

```{r}
n <- 10000 
N <- 1000
# Por MC crudo:
th1 <- numeric(N)
set.seed(1)
for(i in 1:N){
  u <- runif(n)
  th1[i] <- mean(sin(u)^cos(u))
}
# resultado
c(mean(th1),var(th1))
```

En realidad, no necesitamos hacer una doble iteración, al menos para estimar la reducción de varianza. Basta con que tomemos una muestra de $n$ valores y calculemos la varianza de $\hat{\theta}$ de esas $n$ observaciones, tal cual como lo haríamos para estimar una media $\bar{X}$ y la varianza de la media, que es simplemente $s^2/n$. Eso es lo que haremos ahora, con lo que nuestro ejercicio se simplifica considerablemente. 

```{r}
set.seed(1)
n <- 10000  #tamaño de muestra
h <- function(x)sin(x)^cos(x)
u <- runif(n)
var1 <- var(h(u))
c(mean(h(u)),var1)

```

El algortimo para aplicar variables antitéticas para estimar $\theta = E(h(X))$ en este caso es:

1. generamos $n$ uniformes $u_1,\ldots, u_{2n}$
2. calculamos $Y_i^{(1)} =h(U_i)$ y $Y_i^{(1)} =h(1-U_i)$
3. definimos $Z_i = (Y_i^{(1)} + Y_i^{(2)})/2$.
4. calculamos $\hat(\theta) = \frac{1}{n}\sum_{i=1}^nZ_i$ y $\widehat{var(\theta))}= \frac{1}{n-1}\sum_{i=1}^n(Z_i-\bar{Z})^2$


```{r}
# Por Variables antitéticas:
set.seed(1)
u <- runif(n)
Z <- (h(u) + h(1-u))/2

# var2:
var2 <- var(Z)
c(mean(Z),var2)

# ganancia en varianza
100*(var1-var2)/var1
```


# Variables de control

1. Usar variables de control para estimar $\int_0^1\frac{e^{-x}}{1+x^2}\, dx$

```{r}
h1 <- function(x)exp(-x)/(1+x^2)
h2 <- function(x) x 
u <- runif(100)
plot(h1(u),h2(u))
# Tomemos como variable de control Y=x, E(Y)=0.5
#Se hace un piloto para estimar a
x <- exp(-u)/(1+u^2)
y <- u
A <- cov(cbind(x,y))
a <- A[1,2]/A[2,2]  # estimador de a

set.seed(20)
n <- 10000
xc <- numeric(N)
for(i in 1:N){
  u <- runif(n)
  xc[i] <- mean(h1(u) - a*(u-0.5))
}  
  c(mean(xc),var(xc))
  
# Monte Carlo crudo
set.seed(20)
n <- 10000
x <- numeric(N)
for(i in 1:N){
  u <- runif(n)
  x[i] <- mean(h1(u))
}  
  c(mean(x),var(x))

```

2. Estimar $\theta = E\left[e^{(U+W)^2}\right]$ con $U,W$ iid $\mathcal{U}(0,1)$, comparar contra el método tradicional y determinar si hay reducción y cuánto. 

```{r}
# Por MC crudo:

# var1:

# Por Variables de control

# var2:
```

# Condicionamiento

1. Sea $W\sim Poisson(10)$ y $(X|W=w) \sim Beta(w,w^2+1)$. Encontrar la media no condicional $\theta = E(X)$.

```{r}

```

2. Un proyecto de construcción tiene una duración $X|(\mu,\sigma^2) \sim \mathcal{N}(\mu,\sigma^2)$ donde $\mu \sim \mathcal{N}(100,16)$ y $\sigma^2\sim exp(1/4)$, con $\mu \perp  \sigma^2$. La compañía que construye debe pagar 1000 USD por cada día (y prorata por partes de día) que la duración del proyecto excede $K$ días. ¿Cuál es el costo esperado del retraso?

- Calcular usando MC usual y usando condicionamiento.
- Comparar varianzas

```{r}

```

# Muestreo de importancia
1. Consideren $\int_0^1\frac{e^{-x}}{1+x^2}\, dx$ y cuatro funciones de importancia:

- $g_0(x) = I_{[0,1]}(x)$
- $g_1(x) = e^{-x}$
- $g_2(x) = \frac{\pi}{1+x^2}$
- $g_3(x) = \frac{e^{-x}}{1-e^{-1}}$

```{r}
h <- function(x)exp(-x-log(1+x^2))*(x>0)*(x<1)
g0 <- function(x)ifelse(x>0 & x<1,1,0)
g1 <- function(x)exp(-x)
g2 <- function(x)pi/(1+x^2)
g3 <- function(x)exp(-x)/(1-exp(-1))

#gráficas
curve(h(x),from=0.001, to=0.999, lwd=2, ylim=c(0,1))
curve(h(x)/g1(x),from=0.001,to=0.999,col="red",add=T)
curve(h(x)/g2(x),from=0.001,to=0.999,col="green",add=T)
curve(h(x)/g3(x),from=0.001,to=0.999,col="blue",add=T)
```

2. Calculen $P(X>20)$ cuando $X\sim N(0,1)$.


# Muestreo estratificado

1. Consideren otra vez $\int_0^1\frac{e^{-x}}{1+x^2}\, dx$ y apliquen muestreo estratificado. 

```{r}

```

